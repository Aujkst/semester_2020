---
title: STA403-1 第五次作业
author: 金科 201756010
output:
  
  html_document: default
  word_document: default
  pdf_document:
    includes:
      in_header: header.tex
    keep_tex: yes   
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 第九周：P232第9题

```{r message=FALSE, warning=FALSE}
library(ISLR)
data <- OJ
```

（a）创建一个包含800观测的随机样本的训练集，其余样本建立测试集

```{r }
set.seed(1023)
index <- sample(1:nrow(data), 800)

train_data <- data[index, ]
test_data <- data[-index, ]
```

（b）对训练集建立一棵树，将`Purchase`作为响应变量，其余的变量作为预测变量。用函数`summary()`得到树的汇总统计数据，描述得到的结果。训练错误率是多少？树的终端结点数是多少？
```{r message=FALSE, warning=FALSE}
library(tree)
# names(train_data)
```

```{r }
tree_model <- tree(Purchase ~ ., data = train_data)
summary(tree_model)
```

训练错误率为16.5%，一共有7个终端节点。

（c）输入树对象名称以得到详细的输出文本。选择其中一个终端结点，解释得到的信息。
```{r }
tree_model
```

对于终端结点（7），对应的变量为`LoyalCH`，分裂条件为该变量的值大于0.745157，一共有272个观测被归入该结点，其中96.324%的响应为“CH”，因此预测的结果为CH，偏差为85.69。

（d）对树画图并解释其结果
```{r }
plot(tree_model)
text(tree_model, pretty = 0, cex = 0.7)
```

从图中可以看出变量`LoyalCH`是树中最重要的变量；而位于树左侧的分支，无论观测在该处被分向哪个终端结点，预测结果都为`MM`。

（e）预测测试数据的响应值，并建立混淆矩阵比较真值和预测值。测试错误率是多少？
```{r }
# 混淆矩阵
predict_Y <- predict(tree_model, newdata = test_data, type = "class")
table <- table(predict_Y, test_data[, 1])
table

# 测试错误率
sum(table[c(1,4)]) / sum(table)
```

（f）在训练集上用函数`cv.tree()`确定最优的树规模
```{r }
set.seed(1023)
tree_cv_model <- cv.tree(tree_model, FUN = prune.misclass)
tree_cv_model
```

当终端结点数为7时交叉错误率最低。

（g）画图，x轴表示树的规模，y轴表示交叉验证的分类错误率。
```{r }
plot(tree_cv_model$size, tree_cv_model$dev, 
     type = "b", xlab = "Tree Size", ylab = "Deviance")
```

（h）终端结点数为7时交叉验证分类错误率是最低的，但是树规模从4开始模型偏差的下降就不再明显。

（i）建立一棵有4个终端结点的树。
```{r }
tree_model_prune <- prune.misclass(tree_model, best = 4)
plot(tree_model_prune)
text(tree_model_prune, pretty = 0, cex = 0.9)
```

（j）比较剪枝前后的训练错误率
```{r message=FALSE, warning=FALSE}
sapply(list(Tree = tree_model, 
            Prume = tree_model_prune), 
       function(model) {
         pred <- predict(model, type = "class")
         table <- table(pred, train_data[, 1])
         sum(table[c(2, 3)]) / sum(table)
       })
```

剪枝后的训练误差上升了0.0025。

（k）比较剪枝前后的测试错误率
```{r }
sapply(list(Tree = tree_model, 
            Prume = tree_model_prune), 
       function(model) {
         pred <- predict(model, newdata = test_data, type = "class")
         table <- table(pred, test_data[, 1])
         sum(table[c(2, 3)]) / sum(table)
       })
```

虽然训练误差上升了，但是剪枝后的测试误差下降了0.0074074，说明树模型的泛化能力提高了。

### 第十周：P232第10题

用提升法在`Hitters`数据集上预测`Salary`
```{r message=FALSE, warning=FALSE}
library(ISLR)
data <- Hitters
```

（a）剔除缺失`Salary`信息的观测值，再对Salary作对数变换
```{r }
data <- data[-which(is.na(data$Salary)), ]
# sum(is.na(data$Salary))   # = 0
data$Salary <- log(data$Salary)
```

（b）创建一个有200个观测值的训练集，用余下的观测值建立测试集
```{r }
set.seed(1)
index <- sample(dim(data)[1], 200)
train_data <- data[index, ]
test_data <- data[-index, ]
```

（c）用1000棵树对训练集执行提升法，选择不同的压缩参数\lambda
```{r message=FALSE, warning=FALSE}
library(gbm)
```
```{r }
train_mse <- sapply(10^seq(-5, 0.1, 0.1), function(lambda) {
  set.seed(1)
  model <- gbm(Salary ~ ., data = train_data, 
               distribution = "gaussian", 
               n.trees = 1000, 
               shrinkage = lambda)
  predict <- predict(model, n.trees = 1000)
  mean((predict - train_data$Salary)^2)
})

plot(10^seq(-5, 0.1, 0.1), train_mse, type = "b", 
     xlab = "Lambda", ylab = "MSE", 
     main = "Train set MSE")
```

（d）画出x轴上不同的压缩参数值和y轴上对应的测试均方误差
```{r }
test_mse <- sapply(10^seq(-5, 0, 0.1), function(lambda) {
  set.seed(1)
  model <- gbm(Salary ~ ., data = train_data, 
               distribution = "gaussian", 
               n.trees = 1000, 
               shrinkage = lambda)
  predict <- predict(model, newdata = test_data, n.trees = 1000)
  mean((predict - test_data$Salary)^2)
})
```
```{r }
# 绘图
plot(10^seq(-5, 0, 0.1), test_mse, type = "b", 
     xlab = "Lambda", ylab = "MSE", 
     main = "Test set MSE", pch = 20)
```

（e）比较提升法的测试均方误差与第3、6章中两种回归方法的测试均方误差
```{r message=FALSE, warning=FALSE}
library(glmnet)
```
```{r message=FALSE, warning=FALSE}
# OLS
model_lm <- lm(Salary ~ ., data = train_data)
predict_lm <- predict(model_lm, newdata = test_data)

# Lasso & Ridge
train_X <- model.matrix(Salary ~ ., data = train_data)[, -1]
test_X <- model.matrix(Salary ~ ., data = test_data)[, -1]
train_Y <- train_data$Salary
model_ridge <- glmnet(train_X, train_Y, alpha = 0, 
                      lambda = 10^seq(-10, 10, 0.1))
model_lasso <- glmnet(train_X, train_Y, alpha = 1, 
                      lambda = 10^seq(-10, 10, 0.1))
predict_ridge <- predict(model_ridge, 
                         s = (cv.glmnet(train_X, 
                                        train_Y, 
                                        alpha = 0))$lambda.min, 
                         newx = test_X)
predict_lasso <- predict(model_lasso, 
                         s = (cv.glmnet(train_X, 
                                        train_Y, 
                                        alpha = 1))$lambda.min, 
                         newx = test_X)
```
```{r }
c(sapply(list(OLS = predict_lm, 
            Ridge = predict_ridge, 
            Lasso = predict_lasso), 
         function(x) {mean((x - test_data$Salary)^2)}), 
  Boosting = min(test_mse))
```

高下立判，提升法的测试均方误差比另外几种回归方法低得多。

（f）在提升法模型中，哪些变量是最重要的预测变量？
```{r }
bestLambda <- (10^seq(-5, 0, 0.1))[which.min(min(test_mse))]
set.seed(1)
boosting_model <- gbm(Salary ~ ., data = train_data, 
             distribution = "gaussian", 
             n.trees = 1000, 
             shrinkage = bestLambda)
summary(boosting_model)
```

可见`CAtBat`、`CRuns`和`CHits`是最重要的三个变量

（g）现在对训练集使用袋装法。这种方法的测试均方误差是多少？
```{r message=FALSE, warning=FALSE}
library(randomForest)
```
```{r }
set.seed(1)
tree_bagging <- randomForest(Salary ~ ., data = train_data, 
                             mtry = length(train_data) - 1, 
                             importance = TRUE)
predict_bagging <- predict(tree_bagging, newdata = test_data)
mean((predict_bagging - test_data$Salary)^2)
```

