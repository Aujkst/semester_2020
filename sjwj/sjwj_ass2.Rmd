---
title: STA403-1 第二次作业
author: 金科 201756010
output:
  
  html_document: default
  word_document: default
  pdf_document:
    includes:
      in_header: header.tex
    keep_tex: yes   
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 第三周任务：P119 第10题

（a）描述统计
```{r a, message=FALSE, warning=FALSE}
library(ISLR)
library(corrplot)
data = Weekly
```
```{r }
summary(data)

# 计算相关矩阵
corr <- cor(data[, 1:8])
corrplot(corr, method = 'color', type = 'upper', 
         order = 'hclust', addCoef.col = 'black')
```

如图所示，当前的投资回报率与先前的投资回报率相关性很小，只有变量`Year`和`Volume`存在明显的相关性。

（b）使用`Lag1`到`Lag5`和`Volume`拟合逻辑斯谛回归模型预测`Direction`
```{r b}
model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, 
             data = data, family = binomial)
```
提取模型系数的P值信息显示如下
```{r }
summary(model)$coefficients[, 4]
```
在模型中变量`Lag2`在0.05的水平下显著。

（c）
```{r c}
model_predict <- predict(model, type = "response")
model_predict[model_predict > 0.5] <- "Up"
model_predict[model_predict != "Up"] <- "Down"
table(model_predict, data$Direction)
```
计算模型准确度
```{r }
cat("\n The accuracy is", mean(model_predict == data$Direction))
```

- 真值为“Down”时预测正确的概率为$\frac{54}{54+430} = 11.16\%$； 
- 真值为“Up”时预测正确的概率为$\frac{557}{48+557} = 92.07\%$。

（d）使用1990至2008年的数据作为训练集拟合模型，且只把`Lag2`作为预测变量
```{r d}
# 切分数据集
flag = data$Year %in% 1990:2008
train_data = data[flag, ]
test_data = data[!flag, ]

# 训练逻辑斯谛回归模型
neoModel <- glm(Direction ~ Lag2, data = train_data, family = binomial)

# 在测试集中进行预测
neoModelPredict <- predict(neoModel, test_data, type = "response")
neoModelPredict <- ifelse(neoModelPredict > 0.5, "Up", "Down")

# 计算混淆矩阵
table(neoModelPredict, test_data$Direction)
cat("\n The new accuracy is", 
    mean(neoModelPredict == test_data$Direction))
```

（e）使用LDA重复（d）中过程
```{r e, message=FALSE}
library(MASS)
```
```{r }
ldaModel <- lda(Direction ~ Lag2, data = train_data)
ldaModelPredict <- predict(ldaModel, test_data, 
                           type = "response") $ class

# 计算混淆矩阵
table(ldaModelPredict, test_data$Direction)
cat("\n LDA accuracy is", 
    mean(ldaModelPredict == test_data$Direction))
```

（f）使用QDA重复（d）中过程
```{r f}
qdaModel <- qda(Direction ~ Lag2, data = train_data)
qdaModelPredict <- predict(qdaModel, test_data, 
                           type = "response") $ class

# 计算混淆矩阵
table(qdaModelPredict, test_data$Direction)
cat("\n QDA accuracy is", 
    mean(qdaModelPredict == test_data$Direction))
```
QDA模型竟然将所有数据预测为“Up”！

（g）使用$K=1$的KNN重复（d）中过程
```{r g, message=FALSE}
library(class)
```
```{r }
set.seed(1)
knnModelPredict = knn(as.matrix(train_data$Lag2), 
                      as.matrix(test_data$Lag2), 
                      train_data$Direction, k = 1)

# 计算混淆矩阵
table(knnModelPredict, test_data$Direction)
cat("\n KNN (k=1) accuracy is", 
    mean(knnModelPredict == test_data$Direction))
```

（h）对比各模型结果可知逻辑斯谛回归和LDA的结果最好
```{r }
cat("Logistic    ", mean(neoModelPredict == test_data$Direction), 
    "\n   LDA      ", mean(ldaModelPredict == test_data$Direction), 
    "\n   QDA    ", mean(qdaModelPredict == test_data$Direction), 
    "\n KNN(K=1)    ", mean(knnModelPredict == test_data$Direction))
```

（i）对不同K值应用KNN模型
```{r i, warning=FALSE}
knnResults <- double(0)
n = 1
for (i in c(1:10)) {
    for (k in 1:200) {
        set.seed(i)
        knnModelPredict = knn(as.matrix(train_data$Lag2), 
                      as.matrix(test_data$Lag2), 
                      train_data$Direction, k = k)
        knnResults[n] <- mean(knnModelPredict == test_data$Direction)
        n = n + 1
}
}
knnResults <- data.frame(1:200, knnResults)
names(knnResults) <- c("K", "accuracy")
```
```{r warning=FALSE}

# 将结果可视化
library(ggplot2)
```
```{r message=FALSE, warning=FALSE}
ggplot(knnResults, aes(x = K, y = accuracy)) + geom_boxplot(aes(group = K)) + geom_smooth()
```

如图所示，随着K逐渐增大，模型精确度大体上先增大后减小。其中当K大约取150时精确度达到最大，此时计算混淆矩阵
```{r }
set.seed(1)
knnModelPredict = knn(as.matrix(train_data$Lag2), 
                      as.matrix(test_data$Lag2), 
                      train_data$Direction, k = 150)

# 计算混淆矩阵
table(knnModelPredict, test_data$Direction)
cat("\n KNN (k=150) accuracy is", 
    mean(knnModelPredict == test_data$Direction))
```

在原有LDA模型中增加`Lag1`和`Lag2`^2两项
```{r }
neoLdaModel <- lda(Direction ~ Lag1 + Lag2 + I(Lag2^2), 
                   data = train_data)
neoLdaModelPredict <- predict(neoLdaModel, test_data, 
                              type = "response") $ class

# 计算混淆矩阵
table(neoLdaModelPredict, test_data$Direction)
cat("\n neoLDA accuracy is", 
    mean(neoLdaModelPredict == test_data$Direction))
```

在原有QDA模型中增加`Lag2`和`Lag3`的交互项
```{r }
neoQdaModel <- qda(Direction ~ Lag2 + Lag2:Lag3, 
                   data = train_data)
neoQdaModelPredict <- predict(neoQdaModel, test_data, 
                              type = "response") $ class

# 计算混淆矩阵
table(neoQdaModelPredict, test_data$Direction)
cat("\n neoQDA accuracy is", 
    mean(neoQdaModelPredict == test_data$Direction))
```

***

### 第四周任务（1）：P138 第8题

在一个模拟数据集上使用交叉验证法

(a) 生成的模拟数据集如下：
```{r 2a}
set.seed(1)
y = rnorm(100) 
x = rnorm(100)
y = x - 2 * x^2 + rnorm(100)
```

其中$n=100$、$p=2$，模型为$y=x-2x^2+\varepsilon$。

(b) 作X对Y的散点图，讨论结果
```{r 2b, warning=FALSE}
library(ggplot2)
```
```{r warning=FALSE}
ggplot(data.frame(y,x), aes(x = x, y = y)) + geom_point() + geom_smooth()
```

如图所示，Y与X的图像形如开口向下的抛物线。

(c) 最小二乘法拟合下面四个模型产生的LOOCV误差
- $Y = \beta_0 + \beta_1X + \varepsilon$
- $Y = \beta_0 + \beta_1X + \beta_1X^2 + \varepsilon$
- $Y = \beta_0 + \beta_1X + \beta_1X ^2+ \beta_1X^3 + \varepsilon$
- $Y = \beta_0 + \beta_1X + \beta_1X^2 + \beta_1X^3 + \beta_1X^4 + \varepsilon$
```{r 2c, warning=FALSE}
library(boot)
```
```{r }
loocv_error <- function(data, range) {
  result <- numeric(length(range))
  for (i in range) {
    glm <- glm(y ~ poly(x, i), data = data)
    result[i] <- cv.glm(data, glm)$delta[1]
  }
  return(result)
}

set.seed(1)
error <- loocv_error(data.frame(x,y), 1:4)
error
```

(d) 用另外的随机种子重复步骤(c)
```{r 2d}
set.seed(2)
error <- loocv_error(data.frame(x,y), 1:4)
error
```
设另外的随机种子对LOOCV误差没有影响，因为该方法在训练集和验证集的分割上不存在随机性。

(e) 在步骤(c)的结果中，模型$Y = \beta_0 + \beta_1X + \beta_1X^2 + \varepsilon$具有最小的LOOCV误差(1.086596)，因为模拟函数本身为二次型，过高的阶数会使模型过拟合到随机噪声上，增大泛化误差。

(f) 讨论用最小二乘法拟合(c)中的每个模型所得到的系数估计的统计意义。
```{r 2f}
summary(lm(y ~ poly(x, 4)))
```
结果表明$x$、$x^2$两项的系数在0.05的水平上显著，与交叉验证法得到的结论一致。

***

### 第四周任务（2）：P138 第9题
考虑*MASS*程序包中的*Boston*住房数据集
```{r 3}
library(MASS)
library(boot)
data = Boston
```
(a) 给出一个对`medv`（房价中位数）的总体均值的估计，记为$\hat{\mu}$
```{r 3a}
medv_mean <- mean(data$medv)
medv_mean
```
$\hat{\mu} = 22.53281$。

(b) 给出一个对$\hat{\mu}$的标准误差的估计
```{r 3b}
medv_mean_se <- sd(data$medv) / sqrt(length(data$medv))
medv_mean_se
```
可以认为，对于总体的一个随机样本，平均来说`medv`样本均值与其真值之间相差0.4088611。

(c) 用自助法估计$\hat{\mu}$的标准误差
```{r 3c}
mean_sample <- function(x, index) mean(x[index])
set.seed(1)
medv_boot <- boot(data$medv, statistic = mean_sample, R = 1000)
medv_boot
```
基于自助法可得变量$\hat{\mu}$的标准误差为0.4106622，与刚才(b)中的结果非常接近。


(d) 基于自助法估计给出`medv`均值的95%置信区间，并于t.text()相比较
```{r 3d}
#t.test(data$medv)
#    95 percent confidence interval:
#          21.72953 23.33608

# 打印结果
cat("\n boostrap method: [", 
    medv_boot$t0 - 2 * sd(medv_boot$t), ", ", 
    medv_boot$t0 + 2*sd(medv_boot$t), "]", 
    "\n  t.test method: [", 
    21.72953 , ", ", 23.33608, "]")
```

(e) 基于这个数据集，给出`medv`总体中位数的估计
```{r 3e}
medv_median <- median(Boston$medv)
medv_median
```

(f) 用自助法估计中位数的标准误差
```{r 3f}
median_sample <- function(x, index) median(x[index])
set.seed(1)
medv_median_boot <- boot(data$medv, statistic = median_sample, R = 1000)
medv_median_boot
```

由自助法计算结果可得，中位数估计的标准误差为0.3778075。

(g) 给出波士顿郊区的`medv`的10%分位数的估计
```{r 3g}
medv_qntl <- quantile(data$medv, 0.1);
medv_qntl
```

(h) 用自助法估计`medv`的10%分位数的估计
```{r 3h}
qntl_sample <- function(x, index) quantile(x[index], 0.1)
set.seed(1)
medv_qntl_boot <- boot(data$medv, statistic = qntl_sample, R = 1000)
medv_qntl_boot
```

由自助法计算结果可得，10%分位数估计的标准误差为0.4767526

